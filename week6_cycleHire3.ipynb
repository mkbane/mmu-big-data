{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_cycleHire3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiYuHCXf7uSjn6n8TX1p+g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use previous lab notebooks to install and run a spark instance, and then follow the instructions within \"TO DO\" comments"
      ],
      "metadata": {
        "id": "GQaGTIkvB8p7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZMzK1NEOxG"
      },
      "source": [
        "# TO DO: set-up spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO:  install findspark "
      ],
      "metadata": {
        "id": "x7ep6be3O27B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZguLonUE-js"
      },
      "source": [
        "# TO DO: init spark (ensure SPARK_HOME set to same version as we download)\n",
        "import os\n",
        "\n",
        "\n",
        "# use 'spark' as reference (as per standard approach)\n",
        "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHbddaLQUNwo"
      },
      "source": [
        "# get bike hire file for given year from TfL open data\n",
        "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
        "!unzip cyclehireusagestats-2014.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmIQqnIFGKKl"
      },
      "source": [
        "# TO DO: read a file into a DF called \"j_df\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how many entries (rows) in data\n",
        "numRows = j_df.count()\n",
        "print(\"there are \",numRows,\" rows\")\n"
      ],
      "metadata": {
        "id": "reewcKSOmA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to do a histogram of e.g. duration\n",
        "# there is no graphical element of Spark (CHECK) so need to do locally\n",
        "# we can use Python libs pandas & matplotlib\n",
        "\n",
        "# import Python libs\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "plot_pdf = j_df[[\"Duration\"]].toPandas() # take just \"Duration\" col, convert to pandas dataframe structure ready to plot\n",
        "#print(\"we have \", plot_df.count(),\" elements to plot\")\n",
        "print(\"we use '_pdf' to denote 'pandas data frame'\")\n",
        "\n",
        "plot_pdf.plot(kind=\"hist\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M-RKynHyc_Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph is not very information so we need to wrangle our data..."
      ],
      "metadata": {
        "id": "CfOCCzx5C1m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall...\n",
        "j_df.agg({\"Duration\": \"min\"}).show()\n",
        "j_df.agg({\"Duration\": \"mean\"}).show()\n",
        "j_df.agg({\"Duration\": \"max\"}).show()\n",
        "j_df.agg({\"Duration\": \"skewness\"}).show()"
      ],
      "metadata": {
        "id": "RnbEJq6YcozK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at values (random sampling)\n",
        "sample_df=j_df.sample(0.001) # 0.001 of 0.5M is 500\n",
        "print(\"sample has \",sample_df.count(),\" elements\")\n",
        "plotSample_pdf = sample_df[[\"Bike Id\", \"Duration\"]].toPandas()\n",
        "plotSample_pdf.plot(kind=\"scatter\", y=\"Duration\", x=\"Bike Id\") # scatter plot (duration .v. bikeID)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H47co6TpeJLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so vast majority of rides in this month have \"Duration\" under 2 hours\n",
        "total = j_df.count()\n",
        "numExceed = j_df.filter(\"Duration > 7200\").count()\n",
        "print(\"Of all rides, percentage over 2 hours:\", (numExceed*100)/total)\n",
        "\n"
      ],
      "metadata": {
        "id": "kYyKwVUwbQ4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO DO: add a histogram of ride Duration less or equal to 2 hours"
      ],
      "metadata": {
        "id": "ZKYWze9PDL9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can refine further\n",
        "# e.g. remove rides with Duration of 0 seconds or over 3600\n",
        "rides_df = j_df.filter(\"Duration > 0\").filter(\"Duration <= 3600\")[[\"Duration\"]]\n",
        "# now convert to Pandas df and plot\n",
        "rides_df.toPandas().plot(kind=\"hist\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PkloKZ2ag92j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore the top outliers\n",
        "n = 50\n",
        "threshold = j_df.orderBy(\"Duration\", ascending=False).head(n)[n-1].Duration \n",
        "print(\"Ignoring over \", threshold, \" seconds and less than 1 minute\")"
      ],
      "metadata": {
        "id": "NS3UaVJr_fOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: manually use threshold for given month \n",
        "print(\"histogram with 100 bins and log of frequency for rides lasting more than 5 minutes\")\n",
        "j_df.filter(\"Duration > 300\").filter(\"Duration < xxx\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=100, log=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F4liLYzFnbLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing plots..."
      ],
      "metadata": {
        "id": "Azx0dDUojKIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare plots for jan and part of aug (file 9a)\n",
        "print(\"comparing >=5 min but less than or equal to 2 hours by ploting PDF\")\n",
        "rides_df = j_df.filter(\"Duration >= 300\").filter(\"Duration <= 7200\")[[\"Duration\"]]\n",
        "# now convert to Pandas df and plot\n",
        "rides_df.toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "#plt.ylim((10.0,100000.0))\n",
        "# get y range (so can use in next plot)\n",
        "minY,maxY = plt.ylim()\n",
        "plt.show()\n",
        "\n",
        "aug_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load('./9a*csv'))\n",
        "print(\"for end aug dataset we have \", aug_df.count(), \" rows of data\")\n",
        "aug_df.filter(\"Duration >= 300\").filter(\"Duration < 7200\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "plt.ylim(minY, maxY) # ensure same scales for x- and y- axes\n",
        "plt.show()\n",
        "print(\"i.e. some shift towards longer rides?\")\n"
      ],
      "metadata": {
        "id": "_T0IoSZ8ODN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aug is covered in TWO files\n",
        "# 8b (01Aug to 16Aug)\n",
        "# 9a (17Aug to 31Aug)\n",
        "\n",
        "# let's read in both and plot\n",
        "aug_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(['./8b*csv', './9a*csv']))\n",
        "\n",
        "print(\"ALL of aug has \", aug_df.count(), \" entries\")\n",
        "# attempt to plot\n",
        "aug_df.filter(\"Duration >= 300\").filter(\"Duration < 7200\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "#plt.ylim(minY, maxY)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AZWb9kRH0ihk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what about June (TWO files sort of)\n",
        "june_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(['./6*csv',   # 25May to 21Jun\n",
        "                './7*csv'])) # 22Jun to 19Jul\n",
        "print(\"we have \", aug_df.count(), \" entries\")\n",
        "# attempt to plot\n",
        "june_df.filter(\"Duration >= 300\").filter(\"Duration < 7200\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "#plt.ylim(minY, maxY)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kRu-xjU56ugR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "let us look at UNION: read in each file then append 2nd DF to 1st DF\n",
        "\n"
      ],
      "metadata": {
        "id": "i_icXoIRuCo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using 8b file and sampling\n",
        "temp_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('./8b*csv')\n",
        "\n",
        "# TO DO: how many rows in temp_df?\n",
        "\n",
        "# sample (ratio to obtain) - NB this is not exact ratio\n",
        "# let's take two samples and use UNION\n",
        "\n",
        "sampA_df = temp_df.sample(0.00002)\n",
        "print(\"sample A: \", sampA_df.count())\n",
        "sampA_df.show(sampA_df.count())\n",
        "\n",
        "sampB_df = temp_df.sample(0.00001)\n",
        "print(\"sample B: \", sampB_df.count())\n",
        "sampB_df.show(sampB_df.count())\n"
      ],
      "metadata": {
        "id": "WhLdoJyfsR2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now combine in to one dataframe (same schema (contrast 'join') but keeping all rows of data)\n",
        "# this is a UNION\n",
        "mix_df = sampA_df.union(sampB_df)\n",
        "print(\"mix: \",mix_df.count())\n",
        "mix_df.show(mix_df.count())"
      ],
      "metadata": {
        "id": "wra5GhE1N2tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of some data wrangling\n",
        "\n",
        "NB we expect an \"analysis exception\" when running the next cell"
      ],
      "metadata": {
        "id": "A9xxLwgnIdBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what about june?\n",
        "# let's try reading datasets 6 and 7 then form their union\n",
        "juneA_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load('./6*csv'))  # 25May to 21Jun\n",
        "juneB_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load('./7*csv'))  # 22Jun to 19Jul\n",
        "\n",
        "mix_df = juneA_df.union(juneB_df)\n",
        "print(\"mix: \",mix_df.count())\n",
        "mix_df.show(mix_df.count())"
      ],
      "metadata": {
        "id": "IK7Oomzw08Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the \"Analysis Exception\" carefully\n",
        "# perhaps a clue?\n",
        "\n",
        "# TO DO: print out schema + 2 rows for each juneA/B DF\n",
        "\n",
        "# print scheme directly\n",
        "juneA_df.printSchema()\n",
        "juneB_df.printSchema()"
      ],
      "metadata": {
        "id": "78mi5wdEO2H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so for some reason the '6' file (juneA dataframe) scheme is \"incorrect\"\n",
        "# ... what can we do?\n",
        "\n",
        "# first see if anything in these extra cols that we may need\n",
        "juneA_df[[\"_c9\", \"_c10\", \"_c11\"]].distinct().show()"
      ],
      "metadata": {
        "id": "yA30t8C_7rVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so no actual info, thus we can amend our data set by ignoring these columns and using only \"Duration\" \n",
        "meld_df = juneA_df[[\"Duration\"]].union(juneB_df[[\"Duration\"]]) # i.e. a union of only the 'Duration' columns\n",
        "\n",
        "# now filter the union, convert to Pandas and plot PDF\n",
        "meld_df.filter(\"Duration >= 300\").filter(\"Duration < 7200\").toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "#plt.ylim(minY, maxY)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oNINR4Jb2xqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter by timestamp (& using 'alias' for convenience)"
      ],
      "metadata": {
        "id": "RwyxmW7lJB6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we only want June (not May nor July which were included in the input files)\n",
        "\n",
        "# conversion to expected timestamp format so can filter, and alias so easily refer to 'start'\n",
        "temp_df = juneA_df.select(to_timestamp(col(\"Start Date\"), format=\"dd/MM/yyyy HH:mm\").alias(\"start\"), \"Duration\")\n",
        "\n",
        "# example how to filter for dates later than May, outputting example 5 dates for checking\n",
        "# NB at this point temp_df still contains May & June\n",
        "print(\"quick check we do not see any MAY dates\")\n",
        "temp_df.filter(temp_df.start > \"2014-06-01 00:00:00\").show(5)\n",
        "\n",
        "# we can do similarly re juneB_df\n",
        "temp2_df = juneB_df.select(to_timestamp(col('Start Date'), format=\"dd/MM/yyyy HH:mm\").alias(\"start\"), \"Duration\")\n",
        "\n",
        "# do a UNION of only June data from the two datasets keeping only the Duration column\n",
        "# i.e. we do a filter on each temp DF re 'start' then select only 'Duration' col and form the union of those subsets\n",
        "june_df =  temp_df.filter(temp_df.start > \"2014-06-01 00:00:00\")[[\"Duration\"]].union(temp2_df.filter(temp2_df.start < \"2014-07-01 00:00:00\")[[\"Duration\"]])\n",
        "\n",
        "# plot the June-only data for rides at least 5 mins but less than 2 hours (then compare to January, say)\n",
        "print(\"plot PDF of cleansed data for June only, rides>=5m and <2hr\")\n",
        "june_df.filter(\"Duration >= 300\").filter(\"Duration < 7200\").toPandas().plot(kind=\"hist\", bins=23, density=True)\n",
        "plt.xlabel(\"Duration / seconds\")\n",
        "plt.ylabel(\"PDF\")\n",
        "#plt.ylim(minY, maxY)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3aQW9pLlRCkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: plot various months' PDF and gain empirical knowledge of how length of rides varies over the year 2014"
      ],
      "metadata": {
        "id": "sOX2LIQjw00y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}